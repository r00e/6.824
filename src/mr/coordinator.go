package mr

import (
	"io/ioutil"
	"log"
	"math"
	"net"
	"net/http"
	"net/rpc"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"time"
	// "io/ioutil"
)

var mapTasks chan string
var reduceTasks chan int
var turnOff chan TaskType

type taskStatus byte

const (
	unallocated taskStatus = iota
	inqueue
	allocated
	done
)

type void struct{}

var emptyMember void

type Coordinator struct {
	// Your definitions here.
	allFilePieces     map[string]taskStatus
	allReduceTasks    map[int]taskStatus
	activeWorker      map[string]void
	intermediateFiles [][]string
	nReduce           int
	allMapDone        bool
	allReduceDone     bool
	mapTaskNum        int
	RWLock            *sync.RWMutex
	// TurnOff           bool
}

// Your code here -- RPC handlers for the worker to call.
func (c *Coordinator) RegisterWorkerHandler(args *RpcWorkerStatusArgs, reply *RpcWorkerStatusReply) error {
	c.RWLock.Lock()
	defer c.RWLock.Unlock()
	if c.allMapDone && c.allReduceDone {
		c.activeWorker = nil
		return rpc.ServerError("Coordinator is going down...")
	} else {
		uuid := args.Uuid
		// log.Printf("Register worker : %v", uuid)
		c.activeWorker[uuid] = emptyMember
	}
	return nil
}

func (c *Coordinator) TaskHandler(args *TaskRpcArgs, reply *TaskRpcReply) error {
	select {
	case pieceFileName := <-mapTasks:
		c.RWLock.Lock()
		if _, ok := c.activeWorker[args.Uuid]; !ok {
			return rpc.ServerError("worker is dead")
		}
		reply.TaskType = Map
		reply.PieceFileName = pieceFileName
		c.mapTaskNum++
		reply.NReduce = c.nReduce
		reply.MapTaskNum = c.mapTaskNum
		c.allFilePieces[pieceFileName] = allocated
		// log.Printf("%-25v is allocated now...", pieceFileName)
		c.RWLock.Unlock()
		go c.workerTimer(Map, pieceFileName, args.Uuid, reply.MapTaskNum)
		return nil
	case reduceIdx := <-reduceTasks:
		reply.TaskType = Reduce
		reply.ReduceIdx = reduceIdx

		c.RWLock.Lock()
		if _, ok := c.activeWorker[args.Uuid]; !ok {
			return rpc.ServerError("worker is dead")
		}
		reply.NReduce = c.nReduce
		reply.IntermediateFiles = c.intermediateFiles[reduceIdx]
		c.allReduceTasks[reduceIdx] = allocated
		c.RWLock.Unlock()
		go c.workerTimer(Reduce, strconv.Itoa(reduceIdx), "", -1)
		return nil
	case turnOff := <-turnOff:
		reply.TaskType = turnOff
		c.RWLock.Lock()
		if _, ok := c.activeWorker[args.Uuid]; !ok {
			return rpc.ServerError("worker is dead")
		}
		c.RWLock.Unlock()
		go c.workerTimer(TurnOff, "", uuid, -1)
		return nil
	}
}

func (c *Coordinator) workerTimer(taskType TaskType, taskInfo string, uuid string, mapNum int) {
	// Coordinator will wait 10 senconds for each worker
	ticker := time.NewTicker(10 * time.Second)
	defer ticker.Stop()
	for {
		select {
		case <-ticker.C:
			switch taskType {
			case Map:
				filePrefix := "mr-" + uuid + "-" + strconv.Itoa(mapNum)
				// log.Printf("Map %v is dead, filePrefix is %v", mapNum, filePrefix)
				c.RWLock.Lock()
				c.allFilePieces[taskInfo] = inqueue
				// log.Printf("Map task dead... %v", taskInfo)
				// If Map task died
				// Remove all the intermediate files generated by this Map task
				for rowIdx, fileRow := range c.intermediateFiles {
					var tempArr []string
					for _, fileName := range fileRow {
						if !strings.HasPrefix(fileName, filePrefix) {
							tempArr = append(tempArr, fileName)
						}
						// if strings.HasPrefix(fileName, filePrefix) {
						// 	log.Printf("c.intermediateFiles[%v] len is : %v\t", rowIdx, len(c.intermediateFiles[rowIdx]))
						// 	log.Printf("fileIdx is %v\n", fileIdx)
						// 	c.intermediateFiles[rowIdx] = append(c.intermediateFiles[rowIdx][:fileIdx], c.intermediateFiles[rowIdx][fileIdx+1:]...)
						// }
					}
					c.intermediateFiles[rowIdx] = nil
					c.intermediateFiles[rowIdx] = append(c.intermediateFiles[rowIdx], tempArr...)
				}
				// log.Printf("Map %v is dead...\n", uuid)
				delete(c.activeWorker, uuid)
				c.RWLock.Unlock()
				mapTasks <- taskInfo
			case Reduce:
				idx, _ := strconv.Atoi(taskInfo)
				c.RWLock.Lock()
				c.allReduceTasks[idx] = inqueue
				delete(c.activeWorker, uuid)
				c.RWLock.Unlock()
				reduceTasks <- idx
			case TurnOff:
				c.RWLock.Lock()
				// log.Printf("Worker %v is truning off...\n", uuid)
				delete(c.activeWorker, uuid)
				c.RWLock.Unlock()
			}
			return
		default:
			switch taskType {
			case Map:
				c.RWLock.RLock()
				if c.allFilePieces[taskInfo] == done {
					// log.Printf("%v is done in timer", taskInfo)
					c.RWLock.RUnlock()
					return
				} else {
					c.RWLock.RUnlock()
				}
			case Reduce:
				idx, _ := strconv.Atoi(taskInfo)
				c.RWLock.RLock()
				if c.allReduceTasks[idx] == done {
					c.RWLock.RUnlock()
					return
				} else {
					c.RWLock.RUnlock()
				}
			case TurnOff:
				c.RWLock.Lock()
				delete(c.activeWorker, uuid)
				c.RWLock.Unlock()
			}
		}
	}
}

func (c *Coordinator) DeactiveWorkerHandler(args *RpcWorkerStatusArgs, reply *RpcWorkerStatusReply) error {
	c.RWLock.Lock()
	defer c.RWLock.Unlock()
	delete(c.activeWorker, args.Uuid)
	// log.Println(c.activeWorker)

	return nil
}

func (c *Coordinator) TaskDoneHandler(args *RpcTaskDoneArgs, reply *RpcTaskDoneReply) error {
	switch args.TaskType {
	case Map:
		c.RWLock.Lock()
		defer c.RWLock.Unlock()
		c.allFilePieces[args.TaskDoneInfo] = done
	case Reduce:
		reduceTaskIdx, _ := strconv.Atoi(args.TaskDoneInfo)
		c.RWLock.Lock()
		defer c.RWLock.Unlock()
		c.allReduceTasks[reduceTaskIdx] = done
	}
	reply.Msg = "Task is marked as done..."
	return nil
}

func (c *Coordinator) IntermediateFileHandler(args *IntermediateFileArgs, reply *IntermediateFileReply) error {
	intermediateFileName := args.IntermediateFileName
	reduceIdx := args.ReduceIdx
	c.RWLock.Lock()
	c.intermediateFiles[reduceIdx] = append(c.intermediateFiles[reduceIdx], intermediateFileName)
	c.RWLock.Unlock()
	// for i, v := range c.intermediateFiles {
	// 	fmt.Printf("%v th intermediateFiles are : %v\n", i, v)
	// }
	reply.Msg = "Intermediate File is processed..."
	return nil
}

//
// an example RPC handler.
//
// the RPC argument and reply types are defined in rpc.go.
//
func (c *Coordinator) Example(args *ExampleArgs, reply *ExampleReply) error {
	reply.Y = args.X + 1
	return nil
}

//
// start a thread that listens for RPCs from worker.go
//
func (c *Coordinator) server() {
	rpc.Register(c)
	rpc.HandleHTTP()
	go c.generateTasks()
	//l, e := net.Listen("tcp", ":1234")
	sockname := coordinatorSock()
	os.Remove(sockname)
	l, e := net.Listen("unix", sockname)
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go http.Serve(l, nil)
}

func (c *Coordinator) generateTasks() {
	allMapDone := false
	for !allMapDone {
		c.RWLock.RLock()
		copy := make(map[string]taskStatus)
		for piece, status := range c.allFilePieces {
			// log.Printf("%-25v status is %v\n", piece, status)
			copy[piece] = status
		}
		c.RWLock.RUnlock()
		for piece, status := range copy {
			if status == unallocated {
				// log.Printf("map task go in queue:     %v\n", piece)
				c.RWLock.Lock()
				c.allFilePieces[piece] = inqueue
				c.RWLock.Unlock()
				mapTasks <- piece
			}
		}
		allMapDone = isAllMapDone(c)
	}

	allReduceDone := false
	for !allReduceDone {
		c.RWLock.RLock()
		copy := make(map[int]taskStatus)
		// log.Println("---------------before-----------")
		for idx, status := range c.allReduceTasks {
			// log.Printf("reduce %v task status is : %v\n", idx, status)
			copy[idx] = status
		}
		c.RWLock.RUnlock()
		for idx, status := range copy {
			if status == unallocated {
				// log.Printf("reduce task go in queue:     %v\n", idx)
				c.RWLock.Lock()
				c.allReduceTasks[idx] = inqueue
				c.RWLock.Unlock()
				reduceTasks <- idx
			}
		}
		allReduceDone = isAllReduceDone(c)
		// c.RWLock.RLock()
		// // log.Println("---------------post-----------")
		// for idx, status := range c.allReduceTasks {
		// 	log.Printf("reduce %v task status is : %v\n", idx, status)
		// }
		// log.Printf("are all reduce done? %v", allReduceDone)
		// c.RWLock.RUnlock()
	}

	allWorkersOff := false
	for !allWorkersOff {
		allWorkersOff = isAllWorkersOff(c)
		if !allWorkersOff {
			turnOff <- TurnOff
		}
	}
}

func isAllWorkersOff(c *Coordinator) bool {
	c.RWLock.RLock()
	defer c.RWLock.RUnlock()
	return len(c.activeWorker) == 0
}

//
// main/mrcoordinator.go calls Done() periodically to find out
// if the entire job has finished.
//
func (c *Coordinator) Done() bool {
	ret := false

	// Your code here.
	c.RWLock.RLock()
	// log.Printf("map completed : %v, reduce completed : %v, workers off : %v\n", c.allMapDone, c.allReduceDone, len(c.activeWorker) == 0)
	ret = c.allMapDone && c.allReduceDone && len(c.activeWorker) == 0
	c.RWLock.RUnlock()

	// log.Printf("ret is : %v", ret)

	return ret
}

//
// create a Coordinator.
// main/mrcoordinator.go calls this function.
// nReduce is the number of reduce tasks to use.
//
func MakeCoordinator(files []string, nReduce int) *Coordinator {
	c := Coordinator{}

	// Your code here.
	c.initServer(files, nReduce)

	c.server()
	return &c
}

func (c *Coordinator) initServer(files []string, nReduce int) {
	mapTasks = make(chan string, 5)
	reduceTasks = make(chan int, 5)
	turnOff = make(chan TaskType, 1)
	c.RWLock = new(sync.RWMutex)
	c.mapTaskNum = 0
	c.activeWorker = make(map[string]void)
	c.allFilePieces = make(map[string]taskStatus)
	c.allReduceTasks = make(map[int]taskStatus)
	c.intermediateFiles = make([][]string, nReduce)
	c.nReduce = nReduce
	c.allMapDone = false
	c.allReduceDone = false
	// c.TurnOff = false
	// c.initFilePieces(files)
	for _, file := range files {
		c.allFilePieces[file] = unallocated
	}
	c.initReduceTasks()
}

func (c *Coordinator) initReduceTasks() {
	for i := 0; i < c.nReduce; i++ {
		c.allReduceTasks[i] = unallocated
	}
}

func isAllReduceDone(c *Coordinator) bool {
	c.RWLock.Lock()
	defer c.RWLock.Unlock()
	for _, status := range c.allReduceTasks {
		if status != done {
			c.allReduceDone = false
			return false
		}
	}
	c.allReduceDone = true
	return true
}

func isAllMapDone(c *Coordinator) bool {
	c.RWLock.Lock()
	defer c.RWLock.Unlock()
	for _, status := range c.allFilePieces {
		if status != done {
			c.allMapDone = false
			return false
		}
	}
	c.allMapDone = true
	return true
}

func (c *Coordinator) initFilePieces(fileNames []string) {
	for _, fileName := range fileNames {
		// log.Printf("file is %v\n", fileName)
		c.splitInputFileIntoPieces(fileName)
	}
}

func (c *Coordinator) splitInputFileIntoPieces(fileName string) {
	file, err := os.Open(fileName)
	if err != nil {
		log.Println(err)
		os.Exit(1)
	}
	defer file.Close()

	fileInfo, _ := file.Stat()
	var fileSize int64 = fileInfo.Size()

	const fileChunk = 1 * (1 << 16) //64KB

	totalPartsNum := uint64(math.Ceil(float64(fileSize) / float64(fileChunk)))
	// log.Printf("Splitting file : %v into %d pieces.\n", fileName, totalPartsNum)
	for i := uint64(0); i < totalPartsNum; i++ {
		pieceSize := int(math.Min(fileChunk, float64(fileSize-int64(i*fileChunk))))
		pieceBuffer := make([]byte, pieceSize)

		file.Read(pieceBuffer)

		pieceName := genPieceName(fileName, i)
		_, err := os.Create(pieceName)

		if err != nil {
			log.Println(err)
			os.Exit(1)
		}

		ioutil.WriteFile(pieceName, pieceBuffer, os.ModeAppend)
		c.allFilePieces[pieceName] = unallocated
		// log.Println("Piece created : ", pieceName)
	}
}

func genPieceName(fileName string, pieceIdx uint64) (pieceName string) {
	nameWithoutExtension := fileName[:len(fileName)-len(filepath.Ext(fileName))]
	extension := filepath.Ext(fileName)
	pieceName = "myPiece_" + nameWithoutExtension + "_" + strconv.FormatUint(pieceIdx, 10) + extension
	return
}
